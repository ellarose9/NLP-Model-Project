# -*- coding: utf-8 -*-
"""Data_Analysis_and_Cleanup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KNGUfk_WdVtsH5WxBYtTngtR1cWZZY9n
"""

#import packages
import os
import random
import string
import nltk
from collections import defaultdict
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn import metrics
import pickle
import pandas as pd
from nltk.corpus import stopwords
nltk.download('punkt')

# Create  tuple with complaints, where the first part is the Main Problem, and the second part is the narrative
# It should look like this: [ (Main Problem, narrative), (Main Problem, narrative)]
# Note, I removed all the commas from the .csv file prior to running this, since we are splitting on ,
 
complaints = pd.read_csv('Test_Data.csv', delimiter=',')
 
docs = [tuple(row) for row in complaints.values]

#Printing the first few to make sure it worked
print(docs[0:5])

#We want to clean up the narrative so that we will only see meaningful words
#First step is to remove puncuation and convert to lower case
def clean_text(text):
  #remove punctuation
  text = text.translate(str.maketrans('','', string.punctuation))
  #convert to lower case
  text = text.lower()
  return text

#Second step is to remove stopwords. Very easy to add more if we notice more meaningless words need to be removed
stop_words = set(nltk.corpus.stopwords.words('english'))
stop_words.add('said')
stop_words.add('mr')
stop_words.add('xxxx')
stop_words.add("i")

def get_tokens(text):
  tokens = word_tokenize(text)
  tokens = tuple((t for t in tokens if not t in stop_words))
  return tokens

#please referece word doc for an explaination on how these main problems were identified using the data
category_label = ["Incorrect Information", "Credit Card or Credit Reporting Help", "Loan or Lease Help","Mortgage Help","Administrative Help","Identity Theft or Fraud","Debit Card Help", "Billing or Debt Disputes","Service Disputes"]

# Analysis of what the most frequently used words are in the narratives
def print_frequency_dist(docs):
  tokens = defaultdict(list)

  # making a big list of all the words in each Main Problem

  for doc in docs:
    doc_problem = doc[0]
    doc_narrative = clean_text(doc[1])

    doc_tokens = get_tokens(doc_narrative)

    tokens[doc_problem].append(doc_tokens)

  for category_label, category_tokens in tokens.items():
    print(category_label)
    fd = FreqDist(category_tokens)
    print(fd.most_common(20))

#Creating a shorter version of the complaint set so that we can see results fast
docs_short = docs[0:2000]

#Calling print_frequency_dist to see how it looks
#This will take a while to run if you do it on the full file
#If you want to see the full result, replace docs_short with docs
print_frequency_dist(docs_short)